{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737340fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#앞으로 강의실은 503호에서 수업함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fceeab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "\n",
    "# 우리는 지금까지 중요한 것만 찾고 있었다. => 연산량 줄고 속도 빨라짐. \n",
    "  # ex)수능영어 풀때 중요한 단어 찾는 것과 같음.\n",
    "# 중요한 키워드 찾아서 빠르게 내용 이해하는 것과 비슷한 맥락.\n",
    "\n",
    "#이제부터 할 것: word representation\n",
    "  # 어떻게 단어를 인식해야 중요한 정보가 extract 될 것인지를 다룬다.\n",
    "  # -> discrete한 방식: 단어 하나하나가 별개의 item으로 들어감 \n",
    "        #빈도수 세기를 주로 한다. ex) king이 3번 나왔다.\n",
    "  #  -> 몇 개인가?\n",
    "        #BoW(순서 고려하지 않고 빈도수만 담아 단어를 자루에 담는다.)\n",
    "        #ngram\n",
    "        #tf-idf\n",
    "        \n",
    "        # 위 세 가지 방법을 이용해서 벡터를 만든다.\n",
    "            \n",
    "  #-> continuous한 방식: sequence 정보가 들어감.\n",
    "        #주변 단어를 더 고려한다. ex) king은 male이고 mad하다 등의 정보가 들어감\n",
    "        # word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2968ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Bag of Words\n",
    "# -- 가설: 텍스트에서 자주 나올수록 중요한 단어.\n",
    "# - 카운트 기반으로 하기 때문에 통계적으로 접근한다. \n",
    "# - 자주 쓰이지만 단점이 있다.\n",
    "#   - 단점:\n",
    "#       문장의 sequence 정보가 없다. 왜냐면 bag에 다 넣을 것이기 때문임.\n",
    "#       문장에서 자주 나오지는 않지만 중요한 단어를 캐치하지 못함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58071188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Science is an overlap between Arts and Science.',\n",
       " 'Generally, Arts graduates are right-brained and Science graduates are left-brained.',\n",
       " 'Excelling in both Arts and Science at a time becomes difficult.',\n",
       " 'Natural Language Processing is a part of Data Science.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\example.txt', encoding='utf-8') as file:\n",
    "    data = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71599903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW 하는 과정\n",
    "#1. bag을 만든다.\n",
    "#2. count를 센다.\n",
    "#3. 숫자를 센다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65cec438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science is an overlap between Arts and Science. Generally, Arts graduates are right-brained and Science graduates are left-brained. Excelling in both Arts and Science at a time becomes difficult. Natural Language Processing is a part of Data Science.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = ' '.join(data) #하나의 string으로 문장을 연결함.\n",
    "bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5751fcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data science is an overlap between arts and science. generally, arts graduates are right-brained and science graduates are left-brained. excelling in both arts and science at a time becomes difficult. natural language processing is a part of data science.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = bag.lower() #Data, data는 같은 단어니까 나중에 중복 제거용으로 소문자화\n",
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2be397d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'an',\n",
       " 'overlap',\n",
       " 'between',\n",
       " 'arts',\n",
       " 'and',\n",
       " 'science',\n",
       " '.',\n",
       " 'generally',\n",
       " ',',\n",
       " 'arts',\n",
       " 'graduates',\n",
       " 'are',\n",
       " 'right-brained',\n",
       " 'and',\n",
       " 'science',\n",
       " 'graduates',\n",
       " 'are',\n",
       " 'left-brained',\n",
       " '.',\n",
       " 'excelling',\n",
       " 'in',\n",
       " 'both',\n",
       " 'arts',\n",
       " 'and',\n",
       " 'science',\n",
       " 'at',\n",
       " 'a',\n",
       " 'time',\n",
       " 'becomes',\n",
       " 'difficult',\n",
       " '.',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'part',\n",
       " 'of',\n",
       " 'data',\n",
       " 'science',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#단어 하나하나씩 세어야 한다.\n",
    "#그래서 bag은 tokenize가 필요해.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(bag) \n",
    "tokens #이 자체가 bag이다. 그냥 token의 list가 bag이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9df8333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'data': 2,\n",
       "         'science': 5,\n",
       "         'is': 2,\n",
       "         'an': 1,\n",
       "         'overlap': 1,\n",
       "         'between': 1,\n",
       "         'arts': 3,\n",
       "         'and': 3,\n",
       "         '.': 4,\n",
       "         'generally': 1,\n",
       "         ',': 1,\n",
       "         'graduates': 2,\n",
       "         'are': 2,\n",
       "         'right-brained': 1,\n",
       "         'left-brained': 1,\n",
       "         'excelling': 1,\n",
       "         'in': 1,\n",
       "         'both': 1,\n",
       "         'at': 1,\n",
       "         'a': 2,\n",
       "         'time': 1,\n",
       "         'becomes': 1,\n",
       "         'difficult': 1,\n",
       "         'natural': 1,\n",
       "         'language': 1,\n",
       "         'processing': 1,\n",
       "         'part': 1,\n",
       "         'of': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#이제 bag이 만들어졌으니 데이터가 각각 몇개인지 세어야 한다.\n",
    "#collections의 Counter을 이용해 각각의 단어가 몇개씩 있는지 세어준다.\n",
    "\n",
    "from collections import Counter #Counter는 각 단어의 개수를 세어준다.\n",
    "Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e59a495a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 5),\n",
       " ('.', 4),\n",
       " ('arts', 3),\n",
       " ('and', 3),\n",
       " ('data', 2),\n",
       " ('is', 2),\n",
       " ('graduates', 2),\n",
       " ('are', 2),\n",
       " ('a', 2),\n",
       " ('an', 1),\n",
       " ('overlap', 1),\n",
       " ('between', 1),\n",
       " ('generally', 1),\n",
       " (',', 1),\n",
       " ('right-brained', 1),\n",
       " ('left-brained', 1),\n",
       " ('excelling', 1),\n",
       " ('in', 1),\n",
       " ('both', 1),\n",
       " ('at', 1),\n",
       " ('time', 1),\n",
       " ('becomes', 1),\n",
       " ('difficult', 1),\n",
       " ('natural', 1),\n",
       " ('language', 1),\n",
       " ('processing', 1),\n",
       " ('part', 1),\n",
       " ('of', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터의 개수를 센 결과를 bow변수에 받음.\n",
    "bow = Counter(tokens) \n",
    "\n",
    "#Counter로 개수 센 것으로도 most_common() 가능\n",
    "#가장 많은 개수를 가진 데이터부터 내림차순으로 보여줌.\n",
    "bow.most_common() \n",
    "#여기서는 science가 가장 많다. \n",
    "#이것을 통해 텍스트 내용이 대략 science에 관한 내용이라고 예측할 수 있다.\n",
    "#그런데 이 문서 자체를 다 숫자로 바꿔버려서 아무런 연산을 할 수가 없다 \n",
    "#--> 문장별로 count하면 각 문장별 숫자열이 나옴. \n",
    "    #그러면 걔네끼리 곱해서 연산을 할 수 있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcf465",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################여기까지는 문서 자체를 그냥 한번에 싹다 bag으로 만들어 버리는 과정 ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60a8a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################여기부터는 문장별로 단어를 count하는 과정 #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e5ffb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'data': 1,\n",
       "          'science': 2,\n",
       "          'is': 1,\n",
       "          'an': 1,\n",
       "          'overlap': 1,\n",
       "          'between': 1,\n",
       "          'arts': 1,\n",
       "          'and': 1,\n",
       "          '.': 1}),\n",
       " Counter({'generally': 1,\n",
       "          ',': 1,\n",
       "          'arts': 1,\n",
       "          'graduates': 2,\n",
       "          'are': 2,\n",
       "          'right-brained': 1,\n",
       "          'and': 1,\n",
       "          'science': 1,\n",
       "          'left-brained': 1,\n",
       "          '.': 1}),\n",
       " Counter({'excelling': 1,\n",
       "          'in': 1,\n",
       "          'both': 1,\n",
       "          'arts': 1,\n",
       "          'and': 1,\n",
       "          'science': 1,\n",
       "          'at': 1,\n",
       "          'a': 1,\n",
       "          'time': 1,\n",
       "          'becomes': 1,\n",
       "          'difficult': 1,\n",
       "          '.': 1}),\n",
       " Counter({'natural': 1,\n",
       "          'language': 1,\n",
       "          'processing': 1,\n",
       "          'is': 1,\n",
       "          'a': 1,\n",
       "          'part': 1,\n",
       "          'of': 1,\n",
       "          'data': 1,\n",
       "          'science': 1,\n",
       "          '.': 1})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장별로 count를 해라.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "line_tokens = [word_tokenize(line.lower()) for line in data] #각 문장을 소문자화한 후 각 문장을 토큰화함\n",
    "token_dict = [Counter(line) for line in line_tokens] #토큰화된 결과인 각 문장의 단어들의 개수를 문장별로 센다\n",
    "token_dict\n",
    "\n",
    "#여기서 숫자만 뽑아서 이 문서랑 저 문서가 유사한지 알아보기 위해 어떤 계산을 하는 것이 '문서 유사도'이다.\n",
    "#근데 여기서 문제가 있다.\n",
    "  # 첫번째 문장과 두번째 문장의 similarity를 곱셈으로 나타낼 수 있다고 가정해보자.\n",
    "  # 첫번째 문장의 첫번째 숫자 1과 두번째 문장의 첫번째 숫자 1은 서로 의미가 다르다.\n",
    "     #첫번째 문장의 첫번째 숫자 1은 'data'를 의미하지만 두번째 문장의 첫번째 숫자 1은 'generally를 의미함'\n",
    "  # 그러니까 무작정 숫자를 만들어내면 안 됨.\n",
    "  # 각 문장의 기준이 서로 같아야 함. 즉, 단어 정렬이 모든 문장에 대해 같아야 함.\n",
    "  # 모든 문장에 대해 단어 정렬을 같게 하려면 어떻게 해야 하는가?\n",
    "  # 전체 텍스트를 중복없이 집합(set)으로 만들어서 순서를 만들어야 한다.\n",
    "  # 이 순서를 바탕으로 각 문장의 단어 개수를 넣되 없는건 0으로 처리함.\n",
    "  # 이렇게 해 줘야 숫자끼리 곱했을 때 의미가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2198906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 2, 2, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#원래 bag이란 개념에는 순서가 없음.\n",
    "#그런데 vector 개념으로 갈 땐 순서를 만들어줘야 함.\n",
    "#한편 딕셔너리는 순서가 없음. 그러므로 우리는 Ordered dictionary를 쓴다.\n",
    "\n",
    "for line in token_dict: #line은 딕셔너리 형태.\n",
    "    print([value for _, value in line.items()]) #딕셔너리 형태인 line에서 value만 뽑아옴.\n",
    "    \n",
    "#현재 만들어진 결과의 문제: 각 숫자가 의미하는 바가 문장끼리 일치하지 않음 & 문장의 길이가 서로 다름.\n",
    "#그래서 이제 전체 데이터에 대한 set을 만들어서 단어 각각에 인덱스를 부여해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99e17dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',',\n",
       " '.',\n",
       " 'a',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'arts',\n",
       " 'at',\n",
       " 'becomes',\n",
       " 'between',\n",
       " 'both',\n",
       " 'data',\n",
       " 'difficult',\n",
       " 'excelling',\n",
       " 'generally',\n",
       " 'graduates',\n",
       " 'in',\n",
       " 'is',\n",
       " 'language',\n",
       " 'left-brained',\n",
       " 'natural',\n",
       " 'of',\n",
       " 'overlap',\n",
       " 'part',\n",
       " 'processing',\n",
       " 'right-brained',\n",
       " 'science',\n",
       " 'time'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#전체 데이터에 대한 set을 만들고 인덱스를 부여함.\n",
    "lexicon = set(tokens) #tokens는 위에서 만든, 모든 문장을 토크나이징 한 전체 데이터. 이 tokens로 set을 만든다.\n",
    "lexicon # 그 결과가 이제 lexicon(어휘 사전)이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0607ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict #OrderedDict는 순서가 있는 딕셔너리.\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8d95325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('becomes', 0),\n",
       "             ('a', 0),\n",
       "             ('part', 0),\n",
       "             ('overlap', 0),\n",
       "             ('right-brained', 0),\n",
       "             ('are', 0),\n",
       "             ('between', 0),\n",
       "             ('.', 0),\n",
       "             ('language', 0),\n",
       "             ('left-brained', 0),\n",
       "             ('natural', 0),\n",
       "             ('in', 0),\n",
       "             ('is', 0),\n",
       "             ('at', 0),\n",
       "             ('generally', 0),\n",
       "             ('data', 0),\n",
       "             ('science', 0),\n",
       "             (',', 0),\n",
       "             ('of', 0),\n",
       "             ('arts', 0),\n",
       "             ('and', 0),\n",
       "             ('an', 0),\n",
       "             ('both', 0),\n",
       "             ('time', 0),\n",
       "             ('processing', 0),\n",
       "             ('excelling', 0),\n",
       "             ('graduates', 0),\n",
       "             ('difficult', 0)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lexicon에서 각 토큰(단어)과 0을 넣은, 순서가 있는 딕셔너리(OrderedDic)를 만든다.\n",
    "zerovec = OrderedDict((token,0) for token in lexicon) \n",
    "zerovec #순서가 있는 딕셔너리를 만듦.\n",
    "#모든 벡터가 이제 이 딕셔너리에 들어있는 단어 순서대로 들어갈 것이다.\n",
    "#벡터가 들어가면서 벡터에 있는 단어는 count가 문장에서 각 단어의 개수대로 바뀌는데,\n",
    "#벡터에 없는 단어는 그대로 0으로 처리함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e38f3307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('becomes', 0),\n",
       "              ('a', 0),\n",
       "              ('part', 0),\n",
       "              ('overlap', 1),\n",
       "              ('right-brained', 0),\n",
       "              ('are', 0),\n",
       "              ('between', 1),\n",
       "              ('.', 1),\n",
       "              ('language', 0),\n",
       "              ('left-brained', 0),\n",
       "              ('natural', 0),\n",
       "              ('in', 0),\n",
       "              ('is', 1),\n",
       "              ('at', 0),\n",
       "              ('generally', 0),\n",
       "              ('data', 1),\n",
       "              ('science', 2),\n",
       "              (',', 0),\n",
       "              ('of', 0),\n",
       "              ('arts', 1),\n",
       "              ('and', 1),\n",
       "              ('an', 1),\n",
       "              ('both', 0),\n",
       "              ('time', 0),\n",
       "              ('processing', 0),\n",
       "              ('excelling', 0),\n",
       "              ('graduates', 0),\n",
       "              ('difficult', 0)]),\n",
       " OrderedDict([('becomes', 0),\n",
       "              ('a', 0),\n",
       "              ('part', 0),\n",
       "              ('overlap', 0),\n",
       "              ('right-brained', 1),\n",
       "              ('are', 2),\n",
       "              ('between', 0),\n",
       "              ('.', 1),\n",
       "              ('language', 0),\n",
       "              ('left-brained', 1),\n",
       "              ('natural', 0),\n",
       "              ('in', 0),\n",
       "              ('is', 0),\n",
       "              ('at', 0),\n",
       "              ('generally', 1),\n",
       "              ('data', 0),\n",
       "              ('science', 1),\n",
       "              (',', 1),\n",
       "              ('of', 0),\n",
       "              ('arts', 1),\n",
       "              ('and', 1),\n",
       "              ('an', 0),\n",
       "              ('both', 0),\n",
       "              ('time', 0),\n",
       "              ('processing', 0),\n",
       "              ('excelling', 0),\n",
       "              ('graduates', 2),\n",
       "              ('difficult', 0)]),\n",
       " OrderedDict([('becomes', 1),\n",
       "              ('a', 1),\n",
       "              ('part', 0),\n",
       "              ('overlap', 0),\n",
       "              ('right-brained', 0),\n",
       "              ('are', 0),\n",
       "              ('between', 0),\n",
       "              ('.', 1),\n",
       "              ('language', 0),\n",
       "              ('left-brained', 0),\n",
       "              ('natural', 0),\n",
       "              ('in', 1),\n",
       "              ('is', 0),\n",
       "              ('at', 1),\n",
       "              ('generally', 0),\n",
       "              ('data', 0),\n",
       "              ('science', 1),\n",
       "              (',', 0),\n",
       "              ('of', 0),\n",
       "              ('arts', 1),\n",
       "              ('and', 1),\n",
       "              ('an', 0),\n",
       "              ('both', 1),\n",
       "              ('time', 1),\n",
       "              ('processing', 0),\n",
       "              ('excelling', 1),\n",
       "              ('graduates', 0),\n",
       "              ('difficult', 1)]),\n",
       " OrderedDict([('becomes', 0),\n",
       "              ('a', 1),\n",
       "              ('part', 1),\n",
       "              ('overlap', 0),\n",
       "              ('right-brained', 0),\n",
       "              ('are', 0),\n",
       "              ('between', 0),\n",
       "              ('.', 1),\n",
       "              ('language', 1),\n",
       "              ('left-brained', 0),\n",
       "              ('natural', 1),\n",
       "              ('in', 0),\n",
       "              ('is', 1),\n",
       "              ('at', 0),\n",
       "              ('generally', 0),\n",
       "              ('data', 1),\n",
       "              ('science', 1),\n",
       "              (',', 0),\n",
       "              ('of', 1),\n",
       "              ('arts', 0),\n",
       "              ('and', 0),\n",
       "              ('an', 0),\n",
       "              ('both', 0),\n",
       "              ('time', 0),\n",
       "              ('processing', 1),\n",
       "              ('excelling', 0),\n",
       "              ('graduates', 0),\n",
       "              ('difficult', 0)])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for line in token_dict: #문장별로, 단어와 그 개수를 담은 Counter을 가져옴. Counter 내부는 딕셔너리 형태.\n",
    "    vector = copy.copy(zerovec) #zerovec을 가져와야 그 안의 단어 순서대로 문장의 단어와 그 개수를 할당할 수 있음.\n",
    "    #print(vector)\n",
    "    for key, value in line.items(): #key는 단어, value는 그 개수\n",
    "        vector[key] = value #zerovec을 복사 붙여넣기 한 vector(얘도 ordered dictionary 형태)에 단어와 그 개수를 할당함.\n",
    "        \n",
    "    result.append(vector) #할당된 결과는 OrderedDict 형태인데 각 문장에 대한 결과를 리스트의 원소로 넣음.\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e266494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1]\n",
      "[0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for line in result: #line은 OrderedDict 형태임.\n",
    "    print([value for _, value in line.items()]) #line의 key와 value에서 value만 뽑아 리스트의 원소로 저장함. 각 문장별로 시행함.\n",
    "    \n",
    "# 이제 벡터 길이도 서로 같고\n",
    "# 벡터의 왼쪽에서 오른쪽 방향으로 각 숫자가 의미하는 단어가 문장에 상관없이 모두 같음.\n",
    "  #모든 문장의 벡터에서 맨 첫번째 숫자(원소)의 의미는 'becomes'라는 단어임.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6521b320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('becomes', 0),\n",
       "             ('a', 0),\n",
       "             ('part', 0),\n",
       "             ('overlap', 0),\n",
       "             ('right-brained', 0),\n",
       "             ('are', 0),\n",
       "             ('between', 0),\n",
       "             ('.', 0),\n",
       "             ('language', 0),\n",
       "             ('left-brained', 0),\n",
       "             ('natural', 0),\n",
       "             ('in', 0),\n",
       "             ('is', 0),\n",
       "             ('at', 0),\n",
       "             ('generally', 0),\n",
       "             ('data', 0),\n",
       "             ('science', 0),\n",
       "             (',', 0),\n",
       "             ('of', 0),\n",
       "             ('arts', 0),\n",
       "             ('and', 0),\n",
       "             ('an', 0),\n",
       "             ('both', 0),\n",
       "             ('time', 0),\n",
       "             ('processing', 0),\n",
       "             ('excelling', 0),\n",
       "             ('graduates', 0),\n",
       "             ('difficult', 0)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#한편 zerovec(모든 데이터의 set에 0을 붙여 ordered dictionary로 만든 것)은 값이 변하지 않음.\n",
    "#얘를 기준으로 각 문장을 이것의 단어 순서에 맞춰 그 개수를 집어넣기 때문에 \n",
    "#zerovec은 값이 바뀌면 안됨.\n",
    "\n",
    "zerovec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77276806",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 여기까지가 단저정렬을 같게 하여 문장별로 BoW(벡터)를 만드는 과정 #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5a64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 여기부터는 불용어를 제거하여 문장별로 BoW를 만드는 과정 ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c7e12ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 2, 0], [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1], [0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\82103\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Q. 불용어를 제거하고 BOW 만들기\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "line_tokens = [word_tokenize(line.lower()) for line in data] #data는 각 문장(string)을 원소로 하는 리스트\n",
    "\n",
    "lst1 = []\n",
    "for line in line_tokens:\n",
    "    lst = []\n",
    "    for k in range(len(line)):\n",
    "        if line[k] not in stopwords and line[k].isalpha():\n",
    "            lst.append(line[k])\n",
    "\n",
    "    lst1.append(lst)\n",
    "lst1\n",
    "\n",
    "tkdict2 = [Counter(line) for line in lst1]\n",
    "\n",
    "lexicon = set(tokens)\n",
    "lexicon = [w for w in lexicon if w not in stopwords and w.isalpha()]\n",
    "zerovec = OrderedDict((token,0) for token in lexicon)\n",
    "\n",
    "result = []\n",
    "for line in tkdict2:\n",
    "    vector = copy.copy(zerovec)\n",
    "    #print(vector)\n",
    "    for key, value in line.items():\n",
    "        vector[key] = value        \n",
    "    result.append(vector)\n",
    "result\n",
    "\n",
    "bow=[list(x.values()) for x in result]\n",
    "print(bow)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "49a46b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like apples\n",
    "# I don't like apples\n",
    "\n",
    "# lexicon: I, like, apples, don't, pears\n",
    "#          1    1      1       0     0\n",
    "#          1    1      0       1     1\n",
    "#         [0    0      0       0     0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3747f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#article2를 가져와서 bow만들기 - 한국어 텍스트임\n",
    "# - tokenize\n",
    "# - 불용어: 문장부호, 조사\n",
    "# - 라인별 벡터로 만들기\n",
    "# - 데이터를 판다스에 넣기\n",
    "\n",
    "# 열: 단어 리스트가 있고\n",
    "# 행: 문서가 전부 들어감\n",
    "\n",
    "#               I  like  apples\n",
    "# I like apples 0   0      1\n",
    "\n",
    "# 이런 형식으로.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1f70e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['새 방역체계인 위드 코로나 과정에서 신종 코로나바이러스 감염증(코로나19) 4차 유행이 지속되는 가운데 7일 신규 확진자 수는 2200명대를 기록했다. 닷새 연속 2000명대다.',\n",
       " '중앙방역대책본부는 이날 0시 기준으로 신규 확진자가 2224명 증가해 누적 37만9935명으로 집계됐다고 밝혔다.',\n",
       " '전일(2248명)보다는 24명 줄었지만 지난 3일 이후 닷새 연속으로 2000명대를 유지하며 확산세를 이어가고 있는 상황이다. 지난달 30일(발표일 기준 10월 31일) 신규 확진자 2061명보다는 163명이 많다.',\n",
       " '단계적 일상회복 계획이 개시된 이달 1일 이후 전국적으로 활동과 모임 등이 급증한 것이 영향을 준 것으로 풀이된다. 쌀쌀해진 날씨로 인해 환기가 부족한 실내활동이 늘어난 점도 영향을 주고 있다.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'.\\article2.txt', encoding = 'utf-8') as file:\n",
    "    data = [line.strip() for line in file.readlines()]\n",
    "data = [line for line in data if len(line)>0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aaf9f2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>적</th>\n",
       "      <th>방역</th>\n",
       "      <th>인해</th>\n",
       "      <th>수</th>\n",
       "      <th>단계</th>\n",
       "      <th>부족한</th>\n",
       "      <th>10월</th>\n",
       "      <th>과정</th>\n",
       "      <th>밝혔다</th>\n",
       "      <th>해</th>\n",
       "      <th>...</th>\n",
       "      <th>된다</th>\n",
       "      <th>새</th>\n",
       "      <th>명대</th>\n",
       "      <th>37만</th>\n",
       "      <th>된</th>\n",
       "      <th>이후</th>\n",
       "      <th>개시</th>\n",
       "      <th>확산</th>\n",
       "      <th>날씨</th>\n",
       "      <th>30일</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>새 방역 체계 위드 코로나 과정 신종 코로나바이러스 감염증 코로나 19 4 차 유행 지속 되는 가운데 7일 신규 확 진자 수 2200 명대 기록 했다 닷새 연속 2000 명대</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>중앙 방역 대책 본부 이 날 0시 기준 신규 확 진자 2224 명 증가 해 누적 37만 9935 명 집계 됐다고 밝혔다</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>전일 2248 명 24 명 줄었지만 지난 3일 이후 닷새 연속 2000 명대 유지 하며 확산 세 이 어가 있는 상황 지난달 30일 발표 일 기준 10월 31일 신규 확 진자 2061 명 163 명 많다</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>단계 적 일상 회복 계획 개시 된 이 달 1일 이후 전국 적 활동 모임 등 급증 것 영향 준 것 풀이 된다 쌀쌀해진 날씨 인해 환기 부족한 실내 활동 늘어난 점도 영향 주고 있다</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    적  방역  인해  수  단계  부족한  \\\n",
       "새 방역 체계 위드 코로나 과정 신종 코로나바이러스 감염증 코로나 19 4 차 유행 ...  0   1   0  1   0    0   \n",
       "중앙 방역 대책 본부 이 날 0시 기준 신규 확 진자 2224 명 증가 해 누적 37...  0   1   0  0   0    0   \n",
       "전일 2248 명 24 명 줄었지만 지난 3일 이후 닷새 연속 2000 명대 유지 하...  0   0   0  0   0    0   \n",
       "단계 적 일상 회복 계획 개시 된 이 달 1일 이후 전국 적 활동 모임 등 급증 것 ...  2   0   1  0   1    1   \n",
       "\n",
       "                                                    10월  과정  밝혔다  해  ...  된다  \\\n",
       "새 방역 체계 위드 코로나 과정 신종 코로나바이러스 감염증 코로나 19 4 차 유행 ...    0   1    0  0  ...   0   \n",
       "중앙 방역 대책 본부 이 날 0시 기준 신규 확 진자 2224 명 증가 해 누적 37...    0   0    1  1  ...   0   \n",
       "전일 2248 명 24 명 줄었지만 지난 3일 이후 닷새 연속 2000 명대 유지 하...    1   0    0  0  ...   0   \n",
       "단계 적 일상 회복 계획 개시 된 이 달 1일 이후 전국 적 활동 모임 등 급증 것 ...    0   0    0  0  ...   1   \n",
       "\n",
       "                                                    새  명대  37만  된  이후  개시  확산  \\\n",
       "새 방역 체계 위드 코로나 과정 신종 코로나바이러스 감염증 코로나 19 4 차 유행 ...  1   2    0  0   0   0   0   \n",
       "중앙 방역 대책 본부 이 날 0시 기준 신규 확 진자 2224 명 증가 해 누적 37...  0   0    1  0   0   0   0   \n",
       "전일 2248 명 24 명 줄었지만 지난 3일 이후 닷새 연속 2000 명대 유지 하...  0   1    0  0   1   0   1   \n",
       "단계 적 일상 회복 계획 개시 된 이 달 1일 이후 전국 적 활동 모임 등 급증 것 ...  0   0    0  1   1   1   0   \n",
       "\n",
       "                                                    날씨  30일  \n",
       "새 방역 체계 위드 코로나 과정 신종 코로나바이러스 감염증 코로나 19 4 차 유행 ...   0    0  \n",
       "중앙 방역 대책 본부 이 날 0시 기준 신규 확 진자 2224 명 증가 해 누적 37...   0    0  \n",
       "전일 2248 명 24 명 줄었지만 지난 3일 이후 닷새 연속 2000 명대 유지 하...   0    1  \n",
       "단계 적 일상 회복 계획 개시 된 이 달 1일 이후 전국 적 활동 모임 등 급증 것 ...   1    0  \n",
       "\n",
       "[4 rows x 97 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "okt.tagset\n",
    "parsed = [okt.pos(w) for w in data]\n",
    "parsed\n",
    "stw = ['Josa', 'Punctuation']\n",
    "\n",
    "ls2 = []\n",
    "for line in parsed:\n",
    "    ls = []\n",
    "    for k in range(len(line)):\n",
    "        if line[k][1] not in stw:\n",
    "            ls.append(line[k][0])\n",
    "    ls2.append(ls)\n",
    "ls2\n",
    "\n",
    "ls3=[' '.join(line) for line in ls2]\n",
    "ls3\n",
    "\n",
    "ls4=[okt.morphs(line) for line in ls3]\n",
    "ls4\n",
    "\n",
    "tkdict3 = [Counter(line) for line in ls4]\n",
    "tkdict3\n",
    "\n",
    "\n",
    "bag = ' '.join(ls3)\n",
    "tokens = okt.morphs(bag)\n",
    "lexicon = set(tokens)\n",
    "lexicon\n",
    "zerovec2 = OrderedDict((token,0) for token in lexicon)\n",
    "zerovec2\n",
    "\n",
    "result = []\n",
    "for line in tkdict3:\n",
    "    vector = copy.copy(zerovec2)\n",
    "    for key, value in line.items():\n",
    "        vector[key] = value\n",
    "    result.append(vector)\n",
    "result\n",
    "\n",
    "bow=[list(x.values()) for x in result]\n",
    "\n",
    "# for line in result:\n",
    "#     print([value for key, value in line.items()])\n",
    "    \n",
    "import pandas as pd\n",
    "pd.DataFrame(bow, index = ls3, columns = lexicon )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6e317999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar: 숫자(점) - 1차원 [2]\n",
    "# vector: 선 -- 속도, 방향이 생긴다. - 2차원 [[2,3],[3,2]]\n",
    "# matrix: 면적 (선이 여러개.) - 3차원 \n",
    "# Tensor: 부피(면적이 여러개) - 4차원\n",
    "\n",
    "\n",
    "#벡터는 방향이 있다. \n",
    "\n",
    "# 벡터와 벡터의 거리 재기\n",
    "# 1. 유클리디안 거리(L2 norm): 두 점 사이의 거리. -- 별로 현실성 없다. \n",
    "# 2. 코사인 유사도(cosine similarity) -- 두 벡터 사이의 각도를 이용해서 구한다. \n",
    "# 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "00544d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퀴즈 외울것:\n",
    "\n",
    "#수업시간에 한 코드 외우기\n",
    "# 그냥하기\n",
    "# 문장분절해서 하기\n",
    "# 불용어 빼서 하기\n",
    "\n",
    "# 코드만 쓰면 된다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de0ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
