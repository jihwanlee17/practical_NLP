{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9e680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # report 관련 주의할 것 #\n",
    "\n",
    "# #논문에서 가장 중요한 부분: abstract와 conclusion\n",
    "# 이 두가지 부분을 통해 내용을 전체적으로 파악할 수 있다.\n",
    "\n",
    "# - 리뷰 순서\n",
    "# 1. 읽고 what, why, how를 정리한다.(요점정리)\n",
    "#   - 어떤 차이점이 있길래 쓰여졌고 왜 쓰여졌고 어떻게 과정이 진행되는가\n",
    "# 2. introduction에서 보면 인용되어있는 논문들을 또 읽는다. 인용된것도 다 읽어야해.\n",
    "# 읽고 정리해. 각각에 대한 1번을 정리해.\n",
    "# Garcia, 2009 [link] what, why, how\n",
    "\n",
    "# => 거시적인 관점에서 인사이트를 얻기 위해서 한다. 제일 중요한 부분.\n",
    "\n",
    "# 3. 1번을 수정, 더 자세히 기술한다.\n",
    "\n",
    "# 4. 실험이 어떻게 이뤄졌고 어떤 특징이 있었는지를 기술한다.\n",
    "#    => 보너스점수는 코드를 쓰는 것! 코드를 돌릴 여력은 안 되는데\n",
    "#       해보고 싶으면 코드에 한줄한줄 주석을 달아서 이해를 했다는게 증명되면\n",
    "#       보너스 점수를 줄 것이다.\n",
    "    \n",
    "# 5. 분석내용 => 비판적으로 내 견해 제시. (이 부분에서는 내가봤을때 좀 아니다, 검증해야 한다)\n",
    "\n",
    "# 6. 전체 요약(200자 정도)\n",
    "\n",
    "#감점요인: 표절, 이해부족, 준비부족.\n",
    "\n",
    "#진도체크: 원하는 사람에 한해서 진도를 체크해줄거다. \n",
    "#- what, why, how를 이런식으로 정리했는데 잘 했나요?\n",
    "#- 너가 생각보다 덜 이해가 안된것같다. 어디부분을 더 정리해야겠다.\n",
    "#- 이런식으로 미리미리 피드백을 받는다. 감점요인을 미리 대비.\n",
    "#- 진도체크는 아마 이메일로 공유가 가능한 구글doc이나 피드백을 남길수있게?\n",
    "   #- 화상미팅도 가능.\n",
    "    \n",
    "#점수 - 점수에 대한 discussion은 안 할거다. 답장 안할거다.\n",
    "# 수학적인 부분은 리뷰에 안 써도 됨. \n",
    "\n",
    "#이걸하는 목적: 리뷰하는 방법 아는 것, 내가 선택한 주제의 전반적인 흐름을 아는 것!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d69e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소분석: tokenize, ngram, pos tagging......\n",
    "# 구문분석: syntax => 우린 안한다. 할게 너무 많다.\n",
    "# 의미분석: semantic analysis (오늘 할 내용!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40769ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 어간: stem (한국어에서 동사, 영어에서 명사, 동사. - 변하지 않는부분.)\n",
    "# - 어미: ending (변하는 부분)\n",
    "  \n",
    "# ex) 달리-다/기/는...., 달리-ㄴ,...\n",
    "    \n",
    "# - 어근: root (변하지 않는 부분)\n",
    "# - 접사: affix (변하는 부분 - 어근에 붙어서 어근의 내용을 제한하는 부분) \n",
    "#     - suffix, prefix, infix,...\n",
    "    \n",
    "# ex) 햇과일/과일, 짓누르다/누르다\n",
    "\n",
    "# #어간 추출: stemming\n",
    "# - 규칙 기반임. pos-tagging을 고려하지 않음. 대부분의 결과가 부정확함.\n",
    "# - -s를 지워라. --> hypothesis => hypothesi\n",
    "# - smiling => smile(동사) => pos tag info 저장되지 않는다.\n",
    "# #표제어 추출: lemmatization\n",
    "# - wordnet 사전을 활용함. pos info 활용함. 사전에 많이 의존한다.\n",
    "# - smiling -> smile(명사)\n",
    "\n",
    "# 왜 하는가?\n",
    "# 1. 연산량이 줄어듦(작업 효율성 증가.)\n",
    "# 2. 효율성 증가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1008d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemmer\n",
    "# - porter, lancaster, regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2cdf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cooking', 'cooked', 'cookery', 'cooker', 'cooks']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "word = 'Cooking cooked cookery cooker cooks'.split()\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d38bfe78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cook', 'cook', 'cookeri', 'cooker', 'cook']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "[porter.stem(w) for w in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf924ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cook', 'cook', 'cookery', 'cook', 'cook']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "[lancaster.stem(w) for w in word]\n",
    "#lancaster는 다른 스테머보다 더 정확하지만 처리속도가 느리다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8803fb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cook', 'cook', 'cookery', 'cooker', 'cooks']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RegExp - customizing\n",
    "from nltk.stem import RegexpStemmer\n",
    "regexp = RegexpStemmer('ing|ed') #괄호안에 넣는 규칙은 뭘 넣어도 상관없음.\n",
    "\n",
    "[regexp.stem(w) for w in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b130e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friend',\n",
       " 'friendship',\n",
       " 'friends',\n",
       " 'friendshipsstable',\n",
       " 'destabilize',\n",
       " 'misunderstanding',\n",
       " 'railroad',\n",
       " 'moonlight',\n",
       " 'football']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = 'friend friendship friends friendships\\\n",
    "stable destabilize misunderstanding railroad moonlight football'.split()\n",
    "\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848ff3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['friend', 'friendship', 'friend', 'friendshipsst', 'destabil', 'misunderstand', 'railroad', 'moonlight', 'footbal']\n",
      "['friend', 'friend', 'friend', 'friendshipsst', 'dest', 'misunderstand', 'railroad', 'moonlight', 'footbal']\n",
      "['friend', 'friendship', 'friends', 'friendshipsstable', 'destabilize', 'misunderstand', 'railroad', 'moonlight', 'football']\n"
     ]
    }
   ],
   "source": [
    "print([porter.stem(w) for w in word_list])\n",
    "print([lancaster.stem(w) for w in word_list])\n",
    "print([regexp.stem(w) for w in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9f14005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#현업에서는 이 세 개의 stemmer을 다 같이 쓴다.\n",
    "\n",
    "#Q. 함수를 정의하라. porter -> lancaster -> regexp 순서를 \n",
    "# 모두 거치는 stemming이라는 함수를 정의하시오.\n",
    "\n",
    "\n",
    "def stemmer(word:str):\n",
    "    '''option 1) 단어가 그대로일 경우, 차례로 세 스테머를 모두 거치게 한다.\n",
    "    '''\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer, LancasterStemmer,RegexpStemmer\n",
    "    porter = PorterStemmer()\n",
    "    lancaster = LancasterStemmer()\n",
    "    regexp = RegexpStemmer('ing|ed') #괄호안에 넣는 규칙은 뭘 넣어도 상관없음.\n",
    "   \n",
    "    #porter stemmer\n",
    "    stemmed = porter.stem(word)\n",
    "    print('1:',stemmed)\n",
    "    \n",
    "    #lancaster stemmer\n",
    "    if stemmed == word:\n",
    "        stemmed = lancaster.stem(word)\n",
    "        print('2:',stemmed)\n",
    "\n",
    "        #regexp stemmer\n",
    "        if stemmed == word:\n",
    "            stemmed = regexp.stem(word)\n",
    "            print('3:',stemmed)\n",
    "\n",
    "    return stemmed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3969b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: doe\n",
      "1: do\n",
      "1: done\n",
      "2: don\n",
      "1: doer\n",
      "2: doer\n",
      "3: doer\n",
      "1: doer\n",
      "1: did\n",
      "2: did\n",
      "3: did\n",
      "1: doingli\n",
      "1: doabl\n",
      "1: ingdo\n",
      "2: ingdo\n",
      "3: do\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['doe', 'do', 'don', 'doer', 'doer', 'did', 'doingli', 'doabl', 'do']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 'does doing done doer doers did doingly doable ingdo'.split()\n",
    "[stemmer(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "730dfc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stemmer2(word:str): #':타입' : 매개변수의 형식을 지정해줌.(안 써도 상관없음.)\n",
    "    '''\n",
    "       option 2) 모든 스테머를 거친다\n",
    "    '''\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer, LancasterStemmer,RegexpStemmer\n",
    "    porter = PorterStemmer()\n",
    "    lancaster = LancasterStemmer()\n",
    "    regexp = RegexpStemmer('ing|ed') #괄호안에 넣는 규칙은 뭘 넣어도 상관없음.\n",
    "    \n",
    "    stemmed = regexp.stem(word)\n",
    "    stemmed = porter.stem(stemmed)\n",
    "    stemmed = lancaster.stem(stemmed)\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6f52124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doe', 'do', 'don', 'doer', 'doer', 'did', 'dol', 'doabl', 'do']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 'does doing done doer doers did doingly doable ingdo'.split()\n",
    "[stemmer2(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc487658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\82103\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "970585b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quiz: text가 들어왔을 때, tokenize -> stemming ->join을 거쳐 다시 텍스트로 변환되도록 하는 함수를 만든다.\n",
    "def token_stem(text:str):\n",
    "    #tokenize\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    #stemming\n",
    "    text = [porter.stem(w) for w in text]\n",
    "    #join\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d5be627",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"While he did not command the same amount of screen time as the six main \\\n",
    "Friends characters, Gunther was and remains a popular figure among fans. \\\n",
    "Gunther worked as a waiter and manager in the show\\'s coffee house, Central Perk.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33ce538d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"while he did not command the same amount of screen time as the six main friend charact , gunther wa and remain a popular figur among fan . gunther work as a waiter and manag in the show 's coffe hous , central perk .\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15850bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quiz: text가 들어왔을 때, tokenize -> stemming ->join 다시 텍스트로 변환되도록 하는 함수를 만든다.\n",
    "#짧은 코드로 함수 만들기(문제와 결과는 위와 동일)\n",
    "\n",
    "def token_stem2(text:str):\n",
    "    return ' '.join([porter.stem(w) for w in word_tokenize(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c79b6f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"while he did not command the same amount of screen time as the six main friend charact , gunther wa and remain a popular figur among fan . gunther work as a waiter and manag in the show 's coffe hous , central perk .\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"While he did not command the same amount of screen time as the six main \\\n",
    "Friends characters, Gunther was and remains a popular figure among fans. \\\n",
    "Gunther worked as a waiter and manager in the show\\'s coffee house, Central Perk.\"\n",
    "token_stem2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "349c1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bail', 'bail', 'bail', 'bail', 'bail', 'bailais', 'bail']\n",
      "\n",
      "['baila', 'bailo', 'baila', 'baila', 'bailamo', 'bailai', 'bailan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\82103\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### snowball stemmer - 13개 다국어 지원\n",
    "\n",
    "nltk.download('stopwords') #불용어 다운받아야 이용가능함.\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer('spanish')\n",
    "\n",
    "spanish_word = 'baila bailo bailas baila bailamos bailais bailan'.split()\n",
    "#baila: 춤추다\n",
    "\n",
    "print([snowball.stem(w) for w in spanish_word])\n",
    "print()\n",
    "print([porter.stem(w) for w in spanish_word])\n",
    "\n",
    "#snowball의 결과가 더 잘 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "059de7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['달리', '다', '달리다', '달리', '고', '달다', '달리다']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#한국어 stemming \n",
    "#konlpy okt 만 있다.\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "otk = Okt()\n",
    "\n",
    "ko_words = '달리다 달리는 달리고 달려서 달리던'\n",
    "otk.morphs(ko_words, stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33b5501d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('달리', 'Noun'),\n",
       " ('다', 'Josa'),\n",
       " ('달리다', 'Verb'),\n",
       " ('달리', 'Noun'),\n",
       " ('고', 'Josa'),\n",
       " ('달다', 'Verb'),\n",
       " ('달리다', 'Verb')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otk.pos(ko_words, stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02daed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "855834f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization: 표제어 추출\n",
    "#am, are, is -> be (원형으로 돌려준다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46160875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\82103\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misunderstanding\n",
      "misunderstand\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('misunderstanding')) #misunderstanding으로 출력\n",
    "print(lemmatizer.lemmatize('misunderstanding', 'v')) #misunderstand로 출력 --> 품사태깅을 해줘야 정확도 올라간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b66a062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('cooking', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60fbb0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cooking'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('cooking', 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15e6fab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greet'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('greeting','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f8ea457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greeting'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('greeting', 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89137520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be', 'be', 'be']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be_verb = ['am','are','is']\n",
    "[lemmatizer.lemmatize(w) for w in be_verb] #['am', 'are', 'is'] 로 그대로 출력됨\n",
    "[lemmatizer.lemmatize(w,'v') for w in be_verb] #품사 v를 태그하면 모두 be로 표제어 추출 잘 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0a03b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am', 'are', 'is']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "[porter.stem(w) for w in be_verb]\n",
    "#stemmer로는 변형 안된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3bd5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wordnet\n",
    "#동의어 사전.\n",
    "# - 15만개 단어 수록됨\n",
    "# - 상위어, 하위어에 관한 정보가 잘 구축되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d07c29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar') # a collection of synonym words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87f5cf78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7f98f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "024de99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "918c7406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb130c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('beryllium.n.01'),\n",
       " Synset('be.v.01'),\n",
       " Synset('be.v.02'),\n",
       " Synset('be.v.03'),\n",
       " Synset('exist.v.01'),\n",
       " Synset('be.v.05'),\n",
       " Synset('equal.v.01'),\n",
       " Synset('constitute.v.01'),\n",
       " Synset('be.v.08'),\n",
       " Synset('embody.v.02'),\n",
       " Synset('be.v.10'),\n",
       " Synset('be.v.11'),\n",
       " Synset('be.v.12'),\n",
       " Synset('cost.v.01')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2da1c006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('be.v.01.be')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('be.v.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "452a4cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('be.v.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ceff3aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John is rich', 'This is not a good answer']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('be.v.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "851a8909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have the quality of being; (copula, used with an adjective or a predicate noun)'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('be.v.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dfbc9588",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd8395d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stopwords : 불용어\n",
    "#불용어: 문장의 의미에 영향을 미치지 않는 단어들.\n",
    "#문장에서 매우 자주 쓰이지만 의미에 영향을 주지 않는 단어이므로 주로 제거된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87c417de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0dbaa8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b9a709a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "490b2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20da16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"While he did not command the same amount of screen time as the six main \\\n",
    "Friends characters, Gunther was and remains a popular figure among fans. \\\n",
    "Gunther worked as a waiter and manager in the show\\'s coffee house, Central Perk.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0181e076",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [w for w in text.split() if w not in stop_words] #텍스트에서 불용어 제거한 나머지 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1df87848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10666666666666667"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_text)/len(text) #전체 텍스트에서 불용어를 제거한 단어들(텍스트 길이)의 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f037d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 텍스트 길이: 100554 / 불용어 제거한 텍스트 길이: 63739\n"
     ]
    }
   ],
   "source": [
    "#Q. brown corpus에서 카테고리를 하나 정해서 \n",
    "# 전체텍스트 길이 vs 불용어 제거한 길이를 비교하시오.\n",
    "\n",
    "from nltk.corpus import brown\n",
    "news = brown.words(categories = 'news')\n",
    "\n",
    "\n",
    "new_text = [w for w in news if w.lower() not in stop_words]\n",
    "\n",
    "print('전체 텍스트 길이:',len(news),'/','불용어 제거한 텍스트 길이:',len(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "068a3023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "76ab844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력된 전체 텍스트에서 불용어를 제거한 텍스트 길이의 비율을 구하는 함수.\n",
    "def content_fraction(text):\n",
    "    new_corpus = [w for w in text if w.lower() not in stop_words]\n",
    "    return round(len(new_corpus)/len(text) ,4) #소수점 넷째자리까지 반올림한 결과\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c230910b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6339"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_fraction(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50e9d16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'figure']\n"
     ]
    }
   ],
   "source": [
    "#불용어 리스트에 원하는 단어 추가하기\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "print()\n",
    "stop_words.append('figure') ### .append('추가할 단어') 형식으로 불용어를 추가하면 된다.\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a1117bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords.words('korean') #error 발생 -> #한국어 stopwords는 nltk의 stropwords에 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a20bb0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#한국어는 교착어임.\n",
    "#따로 정의해줘야 할 필요가 있다.\n",
    "ko_stopwords = ['은','는','이', '가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6855c9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\82103\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'words' corpus 활용하기\n",
    "# 아주 빈번히 사용되는 단어들을 담아놓았다.\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2245e0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f44d52f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100554"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dddd6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_words = words.words()\n",
    "#words.word에 없는 단어는 매우 희귀한 단어임. \n",
    "#unusual_words 변수에 text에 있는 단어 중 희귀한 단어들을 리스트에 담아놓음\n",
    "unusual_words = [w for w in word_tokenize(text) if w.lower() not in basic_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5447d145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\",\n",
       " ',',\n",
       " ',',\n",
       " '.',\n",
       " '.',\n",
       " 'Friends',\n",
       " 'Gunther',\n",
       " 'Gunther',\n",
       " 'characters',\n",
       " 'fans']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(unusual_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "abc4bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be6272f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\python38\\lib\\site-packages (2.5.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "! pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b5eaf8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "speller = Speller()\n",
    "speller('naturll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "61a6face",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this book authored by show ghost and right running\n",
      "\n",
      "we shall learning how to process natural language and extract insights from it.\n",
      "\n",
      "the first four chapter will introduce you to the basics of nl.\n",
      "\n",
      "later chapters will describe how to deal with complex nl projects.\n",
      "\n",
      "if you want to get early access of it, you should book your order now.\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "# spell correction\n",
    "\n",
    "#파일 임포트하는 법\n",
    "# with open('../data_in/file.txt') as f:\n",
    "    #data = f.readlines() #텍스트파일.readlines()는 위에서 한 줄씩 읽어서 각 줄을 리스트의 원소로 담는다.\n",
    "    \n",
    "#파일 임포트했다고 치면,\n",
    "data = ['In this book authored by Sohom Ghosh and Dwight Gunning\\n', 'we shall learnning how to pracess Natueral Language and extract insights from it.\\n','The first four chapter will introduce you to the basics of NLP.\\n', 'Later chapters will describe how to deal with complex NLP prajects.\\n','If you want to get early access of it, you should book your order now.']\n",
    "\n",
    "\n",
    "#word tokenize\n",
    "for word in data:\n",
    "    print(speller(word.lower()))\n",
    "\n",
    "#pracess -> process로 스펠링 고쳐짐.\n",
    "#그런데 고유명사까지 스펠링 고쳐지는 문제가 있다(Sohom Ghosh -> show ghost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cba9d800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this book authored by Show Ghost and Right Running\n",
      "\n",
      "we shall learning how to process Natural Language and extract insights from it.\n",
      "\n",
      "The first four chapter will introduce you to the basics of LP.\n",
      "\n",
      "Later chapters will describe how to deal with complex LP projects.\n",
      "\n",
      "If you want to get early access of it, you should book your order now.\n"
     ]
    }
   ],
   "source": [
    "#word tokenize\n",
    "for word in data:\n",
    "    print(speller(word))\n",
    "    \n",
    "#텍스트 소문자화 하지 않았을 때에도 고유명사 수정되는 문제 여전함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27454dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
